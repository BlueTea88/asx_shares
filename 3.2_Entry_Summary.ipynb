{
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "3.2 Entry Signals - Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Summarise the statistics of fitted entry signals."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import datetime as dt\n",
      "from itertools import product\n",
      "inlib = '/data/2_dataprep/symboldata'\n",
      "worklib = '/scratch' #Ephemeral directory\n",
      "outlib = '/data/3_entry'\n",
      "symbolList = pd.read_pickle('/data/2_dataprep/symbolList.p')"
     ],
     "language": "python",
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Summarise Entry Performance"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Function to summarise entry performance\n",
      "def entrySummary(entry_data,volume_cutoff,volume,volume_rank):\n",
      "    '''\n",
      "    Summarises the performance statistics of an entry signal.\n",
      "    \n",
      "    entry_data: dataset of entry trades\n",
      "    volume_cutoff: specifies the minimum volume cut-off\n",
      "    volume: specifies the volume column name\n",
      "    volume_rank: specifies the volume rank column name\n",
      "    '''    \n",
      "    temp = entry_data.copy()    \n",
      "    temp = temp[temp[volume]>=volume_cutoff] #Remove trades that do not meet the volume cutoff\n",
      "    temp.sort(['symbol','entry_date'],inplace=True)\n",
      "    temp.index = range(temp.shape[0])\n",
      "    \n",
      "    #Extract performance intervals\n",
      "    intervals = [int(col[col.rfind('_')+1:]) for col in temp.columns if col.startswith('p_DER_')]\n",
      "    \n",
      "    #Loop across intervals\n",
      "    for j in intervals:\n",
      "        #Create temp2 dataframe with non-overlapping trades\n",
      "        temp2 = temp[temp['p_DER_'+str(j)].map(lambda x: pd.isnull(x)==False)]\n",
      "        temp2['include'] = False\n",
      "        for i in temp2.index:\n",
      "            if i==temp2.index[0]: \n",
      "                temp2.ix[i,'include'] = True\n",
      "                ret_symbol = temp2.ix[i,'symbol']\n",
      "                ret_date = temp2.ix[i,'entry_date'] + dt.timedelta(days=j)\n",
      "                continue\n",
      "            if temp2.ix[i,'symbol'] != ret_symbol:\n",
      "                temp2.ix[i,'include'] = True\n",
      "                ret_symbol = temp2.ix[i,'symbol']\n",
      "                ret_date = temp2.ix[i,'entry_date'] + dt.timedelta(days=j)\n",
      "                continue\n",
      "            if ret_date > temp2.ix[i,'entry_date']: continue #Same symbol and overlapping trade intervals\n",
      "            temp2.ix[i,'include'] = True\n",
      "            ret_date = temp2.ix[i,'entry_date'] + dt.timedelta(days=j)\n",
      "            \n",
      "        temp2 = temp2[temp2['include']]\n",
      "        \n",
      "        #Format columns        \n",
      "        temp2['interval'] = j\n",
      "        temp2.rename(columns={'p_MFE_'+str(j):'p_MFE',\n",
      "                              'p_MAE_'+str(j):'p_MAE',\n",
      "                              'p_DER_'+str(j):'p_DER'},inplace=True)\n",
      "        temp2['volume_rank_50'] = temp2[volume_rank].map(lambda x: math.floor((x/50+1)*50))        \n",
      "        temp2['quarter'] = temp2['entry_date'].map(lambda x: str(x.year)+'_'+str(x.quarter))\n",
      "        temp2 = temp2[['symbol','GICS','interval','volume_rank_50','quarter',\n",
      "                       'p_MFE','p_MAE','p_DER']]\n",
      "        \n",
      "        #Summarise data\n",
      "        sumgroup = temp2.groupby(['GICS','interval','volume_rank_50','quarter'])\n",
      "        summary = sumgroup[['p_MFE','p_MAE','p_DER']].sum()\n",
      "        count = pd.DataFrame(sumgroup['symbol'].count(),columns=['count'])\n",
      "        summary = summary.join(count)\n",
      "        \n",
      "        #Append final summaries\n",
      "        if j==intervals[0]: final = summary\n",
      "        else: final = final.append(summary)\n",
      "    return final"
     ],
     "language": "python",
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 3,
     "source": [
      "Consecutive Low"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#List of entry signals to summarise\n",
      "entry_signals = ['ConsecutiveLow_4_close',\n",
      "                 'ConsecutiveLow_5_close',\n",
      "                 'ConsecutiveLow_6_close',\n",
      "                 'ConsecutiveLow_7_close']"
     ],
     "language": "python",
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "CPU times: user 2min 27s, sys: 204 ms, total: 2min 27s\nWall time: 2min 27s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "#Loop across entry signals for processing\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    entry_data = pd.read_pickle(outlib+'/'+entry_signal+'.p') #Read data\n",
      "    sum_data = entrySummary(entry_data,80000,'z_vol_avg30','z_volrank_avg30') #Summarise data\n",
      "    sum_data.to_pickle(outlib+'/sum_'+entry_signal+'.p')"
     ],
     "language": "python",
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Append summary datasets\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    sum_data = pd.read_pickle(outlib+'/sum_'+entry_signal+'.p')\n",
      "    sum_data['entry_signal'] = entry_signal\n",
      "    if i==0: final_data = sum_data.copy()\n",
      "    else: final_data = final_data.append(sum_data)\n",
      "    final_data.to_pickle(outlib+'/summary_ConsecutiveLow.p')\n",
      "    final_data.to_csv(outlib+'/summary_ConsecutiveLow.csv')"
     ],
     "language": "python",
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Download the summarised csv file\n",
      "#starcluster get testcluster /data/3_entry/summary_ConsecutiveLow.csv /shares/models/phase_2/data/3_entry/summary_ConsecutiveLow.csv"
     ],
     "language": "python"
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 3,
     "source": [
      "Donchian High"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#List of entry signals to summarise\n",
      "entry_signals = ['DonchianHigh_300_3_30_close',\n",
      "                 'DonchianHigh_350_0_30',\n",
      "                 'DonchianHigh_300_0_30',\n",
      "                 'DonchianHigh_250_0_30',\n",
      "                 'DonchianHigh_200_0_30',\n",
      "                 'DonchianHigh_150_0_30',\n",
      "                 'DonchianHigh_350_3_30',\n",
      "                 'DonchianHigh_300_3_30',\n",
      "                 'DonchianHigh_250_3_30',\n",
      "                 'DonchianHigh_200_3_30',\n",
      "                 'DonchianHigh_150_3_30',\n",
      "                 'DonchianHigh_350_5_30',\n",
      "                 'DonchianHigh_300_5_30',\n",
      "                 'DonchianHigh_250_5_30',\n",
      "                 'DonchianHigh_200_5_30',\n",
      "                 'DonchianHigh_150_5_30',\n",
      "                 'DonchianHigh_350_3_60',\n",
      "                 'DonchianHigh_300_3_60',\n",
      "                 'DonchianHigh_250_3_60',\n",
      "                 'DonchianHigh_200_3_60',\n",
      "                 'DonchianHigh_150_3_60',\n",
      "                 'DonchianHigh_350_5_60',\n",
      "                 'DonchianHigh_300_5_60',\n",
      "                 'DonchianHigh_250_5_60',\n",
      "                 'DonchianHigh_200_5_60',\n",
      "                 'DonchianHigh_150_5_60']"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "%%time\n",
      "#Loop across entry signals for processing\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    entry_data = pd.read_pickle(outlib+'/'+entry_signal+'.p') #Read data\n",
      "    sum_data = entrySummary(entry_data,80000,'z_vol_avg30','z_volrank_avg30') #Summarise data\n",
      "    sum_data.to_pickle(outlib+'/sum_'+entry_signal+'.p')"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Append summary datasets\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    sum_data = pd.read_pickle(outlib+'/sum_'+entry_signal+'.p')\n",
      "    sum_data['entry_signal'] = entry_signal\n",
      "    if i==0: final_data = sum_data.copy()\n",
      "    else: final_data = final_data.append(sum_data)\n",
      "    final_data.to_pickle(outlib+'/summary_DonchianHigh.p')\n",
      "    final_data.to_csv(outlib+'/summary_DonchianHigh.csv')"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Download the summarised csv file\n",
      "#starcluster get testcluster /data/3_entry/summary_entrysignals.csv /shares/models/phase_2/data/3_entry/summary_DonchianHigh.csv"
     ],
     "language": "python"
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 3,
     "source": [
      "Donchian Low"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "entry_signals = ['DonchianLow_150_0_30',\n",
      "                 'DonchianLow_150_3_30',\n",
      "                 'DonchianLow_150_5_30',\n",
      "                 'DonchianLow_150_3_60',\n",
      "                 'DonchianLow_150_5_60',\n",
      "                 'DonchianLow_200_0_30',\n",
      "                 'DonchianLow_200_3_30',\n",
      "                 'DonchianLow_200_5_30',\n",
      "                 'DonchianLow_200_3_60',\n",
      "                 'DonchianLow_200_5_60',\n",
      "                 'DonchianLow_250_0_30',\n",
      "                 'DonchianLow_250_3_30',\n",
      "                 'DonchianLow_250_5_30',\n",
      "                 'DonchianLow_250_3_60',\n",
      "                 'DonchianLow_250_5_60',\n",
      "                 'DonchianLow_300_0_30',\n",
      "                 'DonchianLow_300_3_30',\n",
      "                 'DonchianLow_300_3_30_close',\n",
      "                 'DonchianLow_300_5_30',\n",
      "                 'DonchianLow_300_3_60',\n",
      "                 'DonchianLow_300_5_60',\n",
      "                 'DonchianLow_350_0_30',\n",
      "                 'DonchianLow_350_3_30',\n",
      "                 'DonchianLow_350_5_30',\n",
      "                 'DonchianLow_350_3_60',\n",
      "                 'DonchianLow_350_5_60']"
     ],
     "language": "python",
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "CPU times: user 15.3 s, sys: 16 ms, total: 15.3 s\nWall time: 15.3 s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "#Loop across entry signals for processing\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    entry_data = pd.read_pickle(outlib+'/'+entry_signal+'.p') #Read data\n",
      "    sum_data = entrySummary(entry_data,80000,'z_vol_avg30','z_volrank_avg30') #Summarise data\n",
      "    sum_data.to_pickle(outlib+'/sum_'+entry_signal+'.p')"
     ],
     "language": "python",
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Append summary datasets\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    sum_data = pd.read_pickle(outlib+'/sum_'+entry_signal+'.p')\n",
      "    sum_data['entry_signal'] = entry_signal\n",
      "    if i==0: final_data = sum_data.copy()\n",
      "    else: final_data = final_data.append(sum_data)\n",
      "    final_data.to_pickle(outlib+'/summary_DonchianLow.p')\n",
      "    final_data.to_csv(outlib+'/summary_DonchianLow.csv')"
     ],
     "language": "python",
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Download the summarised csv file\n",
      "#starcluster get testcluster /data/3_entry/summary_DonchianLow.csv /shares/models/phase_2/data/3_entry/summary_DonchianLow.csv"
     ],
     "language": "python"
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 3,
     "source": [
      "MACD Simple"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#List of entry signals to summarise\n",
      "entry_signals = ['MACD_s_24_52_24_pos',\n",
      "                 'MACD_s_24_52_36_pos',\n",
      "                 'MACD_s_24_52_48_pos',\n",
      "                 'MACD_s_24_104_24_pos',\n",
      "                 'MACD_s_24_104_36_pos',\n",
      "                 'MACD_s_24_104_48_pos',\n",
      "                 'MACD_s_36_52_24_pos',\n",
      "                 'MACD_s_36_52_36_pos',\n",
      "                 'MACD_s_36_52_48_pos',\n",
      "                 'MACD_s_36_104_24_pos',\n",
      "                 'MACD_s_36_104_36_pos',\n",
      "                 'MACD_s_36_104_48_pos',\n",
      "                 'MACD_s_24_156_48_pos',\n",
      "                 'MACD_s_24_208_48_pos',\n",
      "                 'MACD_s_24_260_48_pos',\n",
      "                 'MACD_s_24_312_48_pos',\n",
      "                 'MACD_s_36_156_48_pos',\n",
      "                 'MACD_s_36_156_60_pos',\n",
      "                 'MACD_s_48_156_48_pos',\n",
      "                 'MACD_s_48_156_60_pos',\n",
      "                 'MACD_s_60_156_48_pos',\n",
      "                 'MACD_s_60_156_60_pos',\n",
      "                 'MACD_s_36_208_48_pos',\n",
      "                 'MACD_s_36_208_60_pos',\n",
      "                 'MACD_s_48_208_48_pos',\n",
      "                 'MACD_s_48_208_60_pos',\n",
      "                 'MACD_s_60_208_48_pos',\n",
      "                 'MACD_s_60_208_60_pos',\n",
      "                 'MACD_s_36_260_48_pos',\n",
      "                 'MACD_s_36_260_60_pos',\n",
      "                 'MACD_s_48_260_48_pos',\n",
      "                 'MACD_s_48_260_60_pos',\n",
      "                 'MACD_s_60_260_48_pos',\n",
      "                 'MACD_s_60_260_60_pos',\n",
      "                 'MACD_s_36_312_48_pos',\n",
      "                 'MACD_s_36_312_60_pos',\n",
      "                 'MACD_s_48_312_48_pos',\n",
      "                 'MACD_s_48_312_60_pos',\n",
      "                 'MACD_s_60_312_48_pos',\n",
      "                 'MACD_s_60_312_60_pos']"
     ],
     "language": "python",
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "CPU times: user 5min 51s, sys: 372 ms, total: 5min 51s\nWall time: 5min 51s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "#Loop across entry signals for processing\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    entry_data = pd.read_pickle(outlib+'/'+entry_signal+'.p') #Read data\n",
      "    sum_data = entrySummary(entry_data,80000,'z_vol_avg30','z_volrank_avg30') #Summarise data\n",
      "    sum_data.to_pickle(outlib+'/sum_'+entry_signal+'.p')"
     ],
     "language": "python",
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Append summary datasets\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    sum_data = pd.read_pickle(outlib+'/sum_'+entry_signal+'.p')\n",
      "    sum_data['entry_signal'] = entry_signal\n",
      "    if i==0: final_data = sum_data.copy()\n",
      "    else: final_data = final_data.append(sum_data)\n",
      "    final_data.to_pickle(outlib+'/summary_MACD_Simple.p')\n",
      "    final_data.to_csv(outlib+'/summary_MACD_Simple.csv')"
     ],
     "language": "python",
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Download the summarised csv file\n",
      "#starcluster get testcluster /data/3_entry/summary_MACD_Simple.csv /shares/models/phase_2/data/3_entry/summary_MACD_Simple.csv"
     ],
     "language": "python"
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 3,
     "source": [
      "Force Moving Average"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "entry_signals = ['Force_144_0p1_1',\n",
      "                 'Force_144_0p2_1',\n",
      "                 'Force_144_0p3_1',\n",
      "                 'Force_233_0p1_1',\n",
      "                 'Force_233_0p2_1',\n",
      "                 'Force_233_0p3_1',\n",
      "                 'Force_300_0p1_1',\n",
      "                 'Force_300_0p2_1',\n",
      "                 'Force_300_0p3_1',\n",
      "                 'Force_377_0p1_1',\n",
      "                 'Force_377_0p2_1',\n",
      "                 'Force_377_0p3_1']"
     ],
     "language": "python",
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "%%time\n",
      "#Loop across entry signals for processing\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    entry_data = pd.read_pickle(outlib+'/'+entry_signal+'.p') #Read data\n",
      "    sum_data = entrySummary(entry_data,80000,'z_vol_avg30','z_volrank_avg30') #Summarise data\n",
      "    sum_data.to_pickle(outlib+'/sum_'+entry_signal+'.p')"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Append summary datasets\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    sum_data = pd.read_pickle(outlib+'/sum_'+entry_signal+'.p')\n",
      "    sum_data['entry_signal'] = entry_signal\n",
      "    if i==0: final_data = sum_data.copy()\n",
      "    else: final_data = final_data.append(sum_data)\n",
      "    final_data.to_pickle(outlib+'/summary_Force.p')\n",
      "    final_data.to_csv(outlib+'/summary_Force.csv')"
     ],
     "language": "python",
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Download the summarised csv file\n",
      "#starcluster get testcluster /data/3_entry/summary_Force.csv /shares/models/phase_2/data/3_entry/summary_Force.csv"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "'''\n",
      "entry_signals = ['Force2_144_0p1_1_0p5',\n",
      "                 'Force2_144_0p2_1_0p5',\n",
      "                 'Force2_144_0p3_1_0p5',\n",
      "                 'Force2_144_0p4_1_0p5',\n",
      "                 'Force2_144_0p5_1_0p5',\n",
      "                 'Force2_233_0p1_1_0p5',\n",
      "                 'Force2_233_0p2_1_0p5',\n",
      "                 'Force2_233_0p3_1_0p5',\n",
      "                 'Force2_233_0p4_1_0p5',\n",
      "                 'Force2_233_0p5_1_0p5',\n",
      "                 'Force2_300_0p1_1_0p5',\n",
      "                 'Force2_300_0p2_1_0p5',\n",
      "                 'Force2_300_0p3_1_0p5',\n",
      "                 'Force2_300_0p4_1_0p5',\n",
      "                 'Force2_300_0p5_1_0p5',\n",
      "                 'Force2_350_0p1_1_0p5',\n",
      "                 'Force2_350_0p2_1_0p5',\n",
      "                 'Force2_350_0p3_1_0p5',\n",
      "                 'Force2_350_0p4_1_0p5',\n",
      "                 'Force2_350_0p5_1_0p5',\n",
      "                 'Force2_377_0p1_1_0p5',\n",
      "                 'Force2_377_0p2_1_0p5',\n",
      "                 'Force2_377_0p3_1_0p5',\n",
      "                 'Force2_377_0p4_1_0p5',\n",
      "                 'Force2_377_0p5_1_0p5',\n",
      "                 'Force2_144_0p1_1_0',\n",
      "                 'Force2_144_0p2_1_0',\n",
      "                 'Force2_144_0p3_1_0',\n",
      "                 'Force2_144_0p4_1_0',\n",
      "                 'Force2_144_0p5_1_0',\n",
      "                 'Force2_233_0p1_1_0',\n",
      "                 'Force2_233_0p2_1_0',\n",
      "                 'Force2_233_0p3_1_0',\n",
      "                 'Force2_233_0p4_1_0',\n",
      "                 'Force2_233_0p5_1_0',\n",
      "                 'Force2_300_0p1_1_0',\n",
      "                 'Force2_300_0p2_1_0',\n",
      "                 'Force2_300_0p3_1_0',\n",
      "                 'Force2_300_0p4_1_0',\n",
      "                 'Force2_300_0p5_1_0',\n",
      "                 'Force2_350_0p1_1_0',\n",
      "                 'Force2_350_0p2_1_0',\n",
      "                 'Force2_350_0p3_1_0',\n",
      "                 'Force2_350_0p4_1_0',\n",
      "                 'Force2_350_0p5_1_0',\n",
      "                 'Force2_377_0p1_1_0',\n",
      "                 'Force2_377_0p2_1_0',\n",
      "                 'Force2_377_0p3_1_0',\n",
      "                 'Force2_377_0p4_1_0',\n",
      "                 'Force2_377_0p5_1_0',\n",
      "                 'Force2_144_0p1_1_0p3',\n",
      "                 'Force2_144_0p2_1_0p3',\n",
      "                 'Force2_144_0p3_1_0p3',\n",
      "                 'Force2_144_0p4_1_0p3',\n",
      "                 'Force2_144_0p5_1_0p3',\n",
      "                 'Force2_233_0p1_1_0p3',\n",
      "                 'Force2_233_0p2_1_0p3',\n",
      "                 'Force2_233_0p3_1_0p3',\n",
      "                 'Force2_233_0p4_1_0p3',\n",
      "                 'Force2_233_0p5_1_0p3',\n",
      "                 'Force2_300_0p1_1_0p3',\n",
      "                 'Force2_300_0p2_1_0p3',\n",
      "                 'Force2_300_0p3_1_0p3',\n",
      "                 'Force2_300_0p4_1_0p3',\n",
      "                 'Force2_300_0p5_1_0p3',\n",
      "                 'Force2_350_0p1_1_0p3',\n",
      "                 'Force2_350_0p2_1_0p3',\n",
      "                 'Force2_350_0p3_1_0p3',\n",
      "                 'Force2_350_0p4_1_0p3',\n",
      "                 'Force2_350_0p5_1_0p3',\n",
      "                 'Force2_377_0p1_1_0p3',\n",
      "                 'Force2_377_0p2_1_0p3',\n",
      "                 'Force2_377_0p3_1_0p3',\n",
      "                 'Force2_377_0p4_1_0p3',\n",
      "                 'Force2_377_0p5_1_0p3']\n",
      "'''"
     ],
     "language": "python",
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "'''\n",
      "entry_signals = ['Force2_233_0p3_1_0p5',\n",
      "                 'Force2_233_0p4_1_0p5',\n",
      "                 'Force2_350_0p3_1_0p5',\n",
      "                 'Force2_350_0p4_1_0p5',\n",
      "                 'Force2_233_0p3_1_0p7',\n",
      "                 'Force2_233_0p4_1_0p7',\n",
      "                 'Force2_350_0p3_1_0p7',\n",
      "                 'Force2_350_0p4_1_0p7',\n",
      "                 'Force2_233_0p3_1_1',\n",
      "                 'Force2_233_0p4_1_1',\n",
      "                 'Force2_350_0p3_1_1',\n",
      "                 'Force2_350_0p4_1_1',\n",
      "                 'Force2_233_0p3_1_1.2',\n",
      "                 'Force2_233_0p4_1_1.2',\n",
      "                 'Force2_350_0p3_1_1.2',\n",
      "                 'Force2_350_0p4_1_1.2']\n",
      "'''"
     ],
     "language": "python",
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "entry_signals = ['Force2_233_0p4_1_1_close']"
     ],
     "language": "python",
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "stream": "stdout",
       "output_type": "stream",
       "text": [
        "CPU times: user 32.3 s, sys: 128 ms, total: 32.4 s\nWall time: 32.5 s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "#Loop across entry signals for processing\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    entry_data = pd.read_pickle(outlib+'/'+entry_signal+'.p') #Read data\n",
      "    sum_data = entrySummary(entry_data,80000,'z_vol_avg30','z_volrank_avg30') #Summarise data\n",
      "    sum_data.to_pickle(outlib+'/sum_'+entry_signal+'.p')"
     ],
     "language": "python",
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "entry_signals = ['Force2_144_0p3_1_0p5',\n",
      "                 'Force2_144_0p4_1_0p5',\n",
      "                 'Force2_144_0p5_1_0p5',\n",
      "                 'Force2_233_0p3_1_0p5',\n",
      "                 'Force2_233_0p4_1_0p5',\n",
      "                 'Force2_233_0p5_1_0p5',                 \n",
      "                 'Force2_300_0p3_1_0p5',\n",
      "                 'Force2_300_0p4_1_0p5',\n",
      "                 'Force2_300_0p5_1_0p5',                 \n",
      "                 'Force2_350_0p3_1_0p5',\n",
      "                 'Force2_350_0p4_1_0p5',\n",
      "                 'Force2_350_0p5_1_0p5',\n",
      "                 'Force2_144_0p3_1_1',\n",
      "                 'Force2_144_0p4_1_1',\n",
      "                 'Force2_144_0p5_1_1',\n",
      "                 'Force2_233_0p3_1_1',\n",
      "                 'Force2_233_0p4_1_1',\n",
      "                 'Force2_233_0p4_1_1_close',\n",
      "                 'Force2_233_0p5_1_1',\n",
      "                 'Force2_300_0p3_1_1',\n",
      "                 'Force2_300_0p4_1_1',\n",
      "                 'Force2_300_0p5_1_1',                 \n",
      "                 'Force2_350_0p3_1_1',\n",
      "                 'Force2_350_0p4_1_1',\n",
      "                 'Force2_350_0p5_1_1']"
     ],
     "language": "python",
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Append summary datasets\n",
      "for i,entry_signal in enumerate(entry_signals):\n",
      "    sum_data = pd.read_pickle(outlib+'/sum_'+entry_signal+'.p')\n",
      "    sum_data['entry_signal'] = entry_signal\n",
      "    if i==0: final_data = sum_data.copy()\n",
      "    else: final_data = final_data.append(sum_data)\n",
      "    final_data.to_pickle(outlib+'/summary_Force2-3.p')\n",
      "    final_data.to_csv(outlib+'/summary_Force2-3.csv')"
     ],
     "language": "python",
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "#Download the summarised csv file\n",
      "#starcluster get testcluster /data/3_entry/summary_Force2-3.csv /shares/models/phase_2/data/3_entry/summary_Force2-3.csv"
     ],
     "language": "python"
    }
   ]
  }
 ],
 "cells": [],
 "metadata": {
  "name": "3.2_Entry_Summary"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}